{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinaabdollahnejad/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf    \n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model,load_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting framework environment\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "sm.set_framework('tf.keras')\n",
    "keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 480 # height of image # Height and widht of the image has been changed to 480 in the place of 256 that was originally given. when the original size of image 720 * 480 is rescalled to some lower value there is a some data loss. Therefore, smaller the value bigger will be the data losss, so i have increased the value of dimensions to 480*480 instead of 256*256 that was originally given to us with this assignment notebook.\n",
    "W = 480 # width of image  #reason for heights and weights increase is given in above line.\n",
    "\n",
    "'''This function is used to return the list of path for images and masks in\n",
    "sorted order from the given directory respectively.'''\n",
    "# function to return list of image paths and mask paths \n",
    "def process_data(IMG_DIR, MASK_DIR):\n",
    "    images = [os.path.join(IMG_DIR, x) for x in sorted(os.listdir(IMG_DIR))]\n",
    "    masks = [os.path.join(MASK_DIR, x) for x in sorted(os.listdir(MASK_DIR))]\n",
    "\n",
    "    return images, masks\n",
    "\n",
    "'''This function is used to return splitted list of images and corresponding \n",
    "mask paths in train and test by providing test size.'''\n",
    "# function to load data and train test split\n",
    "def load_data(IMG_DIR, MASK_DIR):\n",
    "    X, y = process_data(IMG_DIR, MASK_DIR)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "'''This function is used to read images. It takes image path as input. \n",
    "After reading image it is resized by width and height provide above(480 x 480). \n",
    "Next normalization is done by dividing each values with 255. And the result is returned.'''\n",
    "# function to read image\n",
    "def read_image(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x / 255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "'''This function is used to read masks.'''\n",
    "# function to read mask\n",
    "def read_mask(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x.astype(np.int32)\n",
    "    return x\n",
    "\n",
    "'''This function is used to generate tensorflow data pipeline. \n",
    "The tensorflow data pipeline is mapped to function ‘preprocess’ .'''\n",
    "# function for tensorflow dataset pipeline\n",
    "def tf_dataset(x, y, batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=5000)\n",
    "    dataset = dataset.map(preprocess)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(2)\n",
    "    return dataset\n",
    "\n",
    "'''This function takes image and mask path. \n",
    "It reads the image and mask as provided by paths. \n",
    "Mask is one hot encoded for multi class segmentation (here 4 class).'''\n",
    "# function to read image and mask amd create one hot encoding for mask\n",
    "def preprocess(x, y):\n",
    "    def f(x, y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "\n",
    "        image = read_image(x)\n",
    "        mask = read_mask(y)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.int32])\n",
    "    mask = tf.one_hot(mask, 4, dtype=tf.int32)\n",
    "    image.set_shape([H, W, 3])\n",
    "    mask.set_shape([H, W, 4])\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      " Train: 7812 \n",
      " Test: 1954\n"
     ]
    }
   ],
   "source": [
    "'''RENDER_IMAGE_DIR_PATH: ‘Path of image directory’\n",
    "GROUND_MASK_DIR_PATH: ‘Path of mask directory’\n",
    "\n",
    "Here load_data function is called. This will load the dataset paths and \n",
    "split it into X_train, X_test, y_train, y_test '''\n",
    "\n",
    "RENDER_IMAGE_DIR_PATH = 'data/images/render'\n",
    "GROUND_MASK_DIR_PATH = 'data/images/clean'\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data(RENDER_IMAGE_DIR_PATH, GROUND_MASK_DIR_PATH)\n",
    "print(f\"Dataset:\\n Train: {len(X_train)} \\n Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 23:53:56.700212: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2023-10-22 23:53:56.700240: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-10-22 23:53:56.700244: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-10-22 23:53:56.700488: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-22 23:53:56.700744: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 \n",
    "\n",
    "'''Here the tf_dataset function is called will generate the tensorflow data pipeline.'''\n",
    "# calling tf_dataset\n",
    "train_dataset = tf_dataset(X_train, y_train, batch=batch_size)\n",
    "valid_dataset = tf_dataset(X_test, y_test, batch=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 480, 480, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)       (None, 480, 480, 64)         1792      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)       (None, 480, 480, 64)         36928     ['block1_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)  (None, 240, 240, 64)         0         ['block1_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)       (None, 240, 240, 128)        73856     ['block1_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)       (None, 240, 240, 128)        147584    ['block2_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)  (None, 120, 120, 128)        0         ['block2_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)       (None, 120, 120, 256)        295168    ['block2_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)       (None, 120, 120, 256)        590080    ['block3_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)       (None, 120, 120, 256)        590080    ['block3_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)  (None, 60, 60, 256)          0         ['block3_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)       (None, 60, 60, 512)          1180160   ['block3_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)       (None, 60, 60, 512)          2359808   ['block4_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)       (None, 60, 60, 512)          2359808   ['block4_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)  (None, 30, 30, 512)          0         ['block4_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)       (None, 30, 30, 512)          2359808   ['block4_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)       (None, 30, 30, 512)          2359808   ['block5_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)       (None, 30, 30, 512)          2359808   ['block5_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling2D)  (None, 15, 15, 512)          0         ['block5_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " center_block1_conv (Conv2D  (None, 15, 15, 512)          2359296   ['block5_pool[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " center_block1_bn (BatchNor  (None, 15, 15, 512)          2048      ['center_block1_conv[0][0]']  \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " center_block1_relu (Activa  (None, 15, 15, 512)          0         ['center_block1_bn[0][0]']    \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " center_block2_conv (Conv2D  (None, 15, 15, 512)          2359296   ['center_block1_relu[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " center_block2_bn (BatchNor  (None, 15, 15, 512)          2048      ['center_block2_conv[0][0]']  \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " center_block2_relu (Activa  (None, 15, 15, 512)          0         ['center_block2_bn[0][0]']    \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " decoder_stage0_upsampling   (None, 30, 30, 512)          0         ['center_block2_relu[0][0]']  \n",
      " (UpSampling2D)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_stage0_concat (Con  (None, 30, 30, 1024)         0         ['decoder_stage0_upsampling[0]\n",
      " catenate)                                                          [0]',                         \n",
      "                                                                     'block5_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_stage0a_conv (Conv  (None, 30, 30, 256)          2359296   ['decoder_stage0_concat[0][0]'\n",
      " 2D)                                                                ]                             \n",
      "                                                                                                  \n",
      " decoder_stage0a_bn (BatchN  (None, 30, 30, 256)          1024      ['decoder_stage0a_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage0a_relu (Acti  (None, 30, 30, 256)          0         ['decoder_stage0a_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " decoder_stage0b_conv (Conv  (None, 30, 30, 256)          589824    ['decoder_stage0a_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " decoder_stage0b_bn (BatchN  (None, 30, 30, 256)          1024      ['decoder_stage0b_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage0b_relu (Acti  (None, 30, 30, 256)          0         ['decoder_stage0b_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " decoder_stage1_upsampling   (None, 60, 60, 256)          0         ['decoder_stage0b_relu[0][0]']\n",
      " (UpSampling2D)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_stage1_concat (Con  (None, 60, 60, 768)          0         ['decoder_stage1_upsampling[0]\n",
      " catenate)                                                          [0]',                         \n",
      "                                                                     'block4_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_stage1a_conv (Conv  (None, 60, 60, 128)          884736    ['decoder_stage1_concat[0][0]'\n",
      " 2D)                                                                ]                             \n",
      "                                                                                                  \n",
      " decoder_stage1a_bn (BatchN  (None, 60, 60, 128)          512       ['decoder_stage1a_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage1a_relu (Acti  (None, 60, 60, 128)          0         ['decoder_stage1a_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " decoder_stage1b_conv (Conv  (None, 60, 60, 128)          147456    ['decoder_stage1a_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " decoder_stage1b_bn (BatchN  (None, 60, 60, 128)          512       ['decoder_stage1b_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage1b_relu (Acti  (None, 60, 60, 128)          0         ['decoder_stage1b_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " decoder_stage2_upsampling   (None, 120, 120, 128)        0         ['decoder_stage1b_relu[0][0]']\n",
      " (UpSampling2D)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_stage2_concat (Con  (None, 120, 120, 384)        0         ['decoder_stage2_upsampling[0]\n",
      " catenate)                                                          [0]',                         \n",
      "                                                                     'block3_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_stage2a_conv (Conv  (None, 120, 120, 64)         221184    ['decoder_stage2_concat[0][0]'\n",
      " 2D)                                                                ]                             \n",
      "                                                                                                  \n",
      " decoder_stage2a_bn (BatchN  (None, 120, 120, 64)         256       ['decoder_stage2a_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage2a_relu (Acti  (None, 120, 120, 64)         0         ['decoder_stage2a_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " decoder_stage2b_conv (Conv  (None, 120, 120, 64)         36864     ['decoder_stage2a_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " decoder_stage2b_bn (BatchN  (None, 120, 120, 64)         256       ['decoder_stage2b_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage2b_relu (Acti  (None, 120, 120, 64)         0         ['decoder_stage2b_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " decoder_stage3_upsampling   (None, 240, 240, 64)         0         ['decoder_stage2b_relu[0][0]']\n",
      " (UpSampling2D)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_stage3_concat (Con  (None, 240, 240, 192)        0         ['decoder_stage3_upsampling[0]\n",
      " catenate)                                                          [0]',                         \n",
      "                                                                     'block2_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_stage3a_conv (Conv  (None, 240, 240, 32)         55296     ['decoder_stage3_concat[0][0]'\n",
      " 2D)                                                                ]                             \n",
      "                                                                                                  \n",
      " decoder_stage3a_bn (BatchN  (None, 240, 240, 32)         128       ['decoder_stage3a_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage3a_relu (Acti  (None, 240, 240, 32)         0         ['decoder_stage3a_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " decoder_stage3b_conv (Conv  (None, 240, 240, 32)         9216      ['decoder_stage3a_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " decoder_stage3b_bn (BatchN  (None, 240, 240, 32)         128       ['decoder_stage3b_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage3b_relu (Acti  (None, 240, 240, 32)         0         ['decoder_stage3b_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " decoder_stage4_upsampling   (None, 480, 480, 32)         0         ['decoder_stage3b_relu[0][0]']\n",
      " (UpSampling2D)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_stage4a_conv (Conv  (None, 480, 480, 16)         4608      ['decoder_stage4_upsampling[0]\n",
      " 2D)                                                                [0]']                         \n",
      "                                                                                                  \n",
      " decoder_stage4a_bn (BatchN  (None, 480, 480, 16)         64        ['decoder_stage4a_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage4a_relu (Acti  (None, 480, 480, 16)         0         ['decoder_stage4a_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " decoder_stage4b_conv (Conv  (None, 480, 480, 16)         2304      ['decoder_stage4a_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " decoder_stage4b_bn (BatchN  (None, 480, 480, 16)         64        ['decoder_stage4b_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " decoder_stage4b_relu (Acti  (None, 480, 480, 16)         0         ['decoder_stage4b_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " final_conv (Conv2D)         (None, 480, 480, 4)          580       ['decoder_stage4b_relu[0][0]']\n",
      "                                                                                                  \n",
      " softmax (Activation)        (None, 480, 480, 4)          0         ['final_conv[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23752708 (90.61 MB)\n",
      "Trainable params: 23748676 (90.59 MB)\n",
      "Non-trainable params: 4032 (15.75 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BACKBONE = 'vgg16'\n",
    "input_shape = (480, 480, 3)\n",
    "n_classes = 4\n",
    "activation = 'softmax'\n",
    "\n",
    "# using segmentation_models to create U-net with vgg16 as a backbone\n",
    "# and pretrained imagenet weights\n",
    "\n",
    "# segmentation_model basically will create a mirror image of our backbone as expansion path and add to the contraction path\n",
    "model = sm.Unet(backbone_name = BACKBONE, \n",
    "                input_shape = input_shape, \n",
    "                classes = n_classes, \n",
    "                activation = activation,\n",
    "                encoder_weights = 'imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sinaabdollahnejad/Documents/Image Segmentation/ImageSeg.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m metrics \u001b[39m=\u001b[39m [sm\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mIOUScore(threshold\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m), sm\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mFScore(threshold\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# compiling the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mlegacy\u001b[39m.\u001b[39mAdam(lr, epsilon\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m, decay\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m), \u001b[39m#epsilon and decay are added as a way to increase the performance. epsilon is a very small number to prevent any division by zero in the implementation. decay helps us to increase performance by reducing the momentum of the optimizer. Adam uses Momentum and Adaptive Learning Rates to converge faster.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                metrics \u001b[39m=\u001b[39m metrics)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train_steps \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_train)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m valid_steps \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_test)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping #importing the necessary modules for callbacks\n",
    "from segmentation_models.metrics import iou_score\n",
    "import datetime, os\n",
    "\n",
    "\"\"\" Defining Hyperparameters \"\"\"\n",
    "img_shape = (480, 480, 3) #input shapes, default one with notebook (256,256,3) to presently this (480,480,3). This is done to accomadate the height and width increase in the input image that we have changed in the previous cells above. \n",
    "num_classes = 4\n",
    "lr = 1e-5 #decreased learning rate gives better generalization\n",
    "batch_size = 16 #increasing the batch size can provide better performance\n",
    "epochs = 30 #Increased epochs give better scores\n",
    "\n",
    "\"\"\" Model building and compiling \"\"\"\n",
    "# metrics for result validation\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "               optimizer = tf.keras.optimizers.legacy.Adam(lr, epsilon=1e-8, decay=1e-6), #epsilon and decay are added as a way to increase the performance. epsilon is a very small number to prevent any division by zero in the implementation. decay helps us to increase performance by reducing the momentum of the optimizer. Adam uses Momentum and Adaptive Learning Rates to converge faster.\n",
    "               metrics = metrics)\n",
    "\n",
    "train_steps = len(X_train)//batch_size\n",
    "valid_steps = len(X_test)//batch_size\n",
    "\n",
    "\n",
    "\"\"\" Callbacks \"\"\"\n",
    "#A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc). You can use callbacks to: Write TensorBoard logs after every batch of training to monitor your metrics, Periodically save your model to disk, Do early stopping, Get a view on internal states and statistics of a model during training.\n",
    "\n",
    "current_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=f'models/LunarModel.h5', monitor='val_iou_score', verbose=1, mode='max', save_best_only=True), # Create a callback that saves the model periodically as training moves along the number of epochs.\n",
    "             \n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_iou_score\", mode='max', patience=4, factor=0.1, verbose=1, min_lr=1e-6), #This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.\n",
    "                         \n",
    "             \n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_iou_score\", patience=5, verbose=1, mode='max'),  #Stop training when a monitored metric has stopped improving.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 23:18:24.809673: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/488 [..............................] - ETA: 3:42:09 - loss: 1.0840 - iou_score: 0.0148 - f1-score: 0.0279"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sinaabdollahnejad/Documents/Image Segmentation/ImageSeg.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m'''model.fit is used to train the model'''\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         steps_per_epoch\u001b[39m=\u001b[39;49mtrain_steps,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         validation_data\u001b[39m=\u001b[39;49mvalid_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         validation_steps\u001b[39m=\u001b[39;49mvalid_steps,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/engine/training.py:1789\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1787\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1788\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1789\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1790\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1791\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/utils/tf_utils.py:694\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 694\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/tensorflow/python/util/nest.py:629\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnest.map_structure\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_structure\u001b[39m(func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    545\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \n\u001b[1;32m    547\u001b[0m \u001b[39m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m   \u001b[39mreturn\u001b[39;00m nest_util\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[1;32m    630\u001b[0m       nest_util\u001b[39m.\u001b[39;49mModality\u001b[39m.\u001b[39;49mCORE, func, \u001b[39m*\u001b[39;49mstructure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    631\u001b[0m   )\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/tensorflow/python/util/nest_util.py:1168\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \n\u001b[1;32m   1073\u001b[0m \u001b[39m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mCORE:\n\u001b[0;32m-> 1168\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_core_map_structure(func, \u001b[39m*\u001b[39;49mstructure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1169\u001b[0m \u001b[39melif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mDATA:\n\u001b[1;32m   1170\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/tensorflow/python/util/nest_util.py:1208\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m flat_structure \u001b[39m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m   1204\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m   1206\u001b[0m \u001b[39mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1207\u001b[0m     structure[\u001b[39m0\u001b[39m],\n\u001b[0;32m-> 1208\u001b[0m     [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m   1209\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites,\n\u001b[1;32m   1210\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/tensorflow/python/util/nest_util.py:1208\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1203\u001b[0m flat_structure \u001b[39m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m   1204\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m   1206\u001b[0m \u001b[39mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1207\u001b[0m     structure[\u001b[39m0\u001b[39m],\n\u001b[0;32m-> 1208\u001b[0m     [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m   1209\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites,\n\u001b[1;32m   1210\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/utils/tf_utils.py:687\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    685\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 687\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    688\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:396\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:362\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    361\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m    363\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''model.fit is used to train the model'''\n",
    "model_history = model.fit(train_dataset,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_data=valid_dataset,\n",
    "        validation_steps=valid_steps,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping #importing the necessary modules for callbacks\n",
    "from segmentation_models.metrics import iou_score\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown metric function: 'iou_score'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sinaabdollahnejad/Documents/Image Segmentation/ImageSeg.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sinaabdollahnejad/Documents/Image%20Segmentation/ImageSeg.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39;49m\u001b[39mmodels/LunarModel.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m    263\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    264\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Image Segmentation/myenv/lib/python3.9/site-packages/keras/src/saving/legacy/serialization.py:537\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    535\u001b[0m     obj \u001b[39m=\u001b[39m module_objects\u001b[39m.\u001b[39mget(object_name)\n\u001b[1;32m    536\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    538\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown \u001b[39m\u001b[39m{\u001b[39;00mprintable_module_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mobject_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease ensure you are using a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`keras.utils.custom_object_scope` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    541\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mand that this object is included in the scope. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m#registering_the_custom_object for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         )\n\u001b[1;32m    546\u001b[0m \u001b[39m# Classes passed by name are instantiated with no args, functions are\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[39m# returned as-is.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[39mif\u001b[39;00m tf_inspect\u001b[39m.\u001b[39misclass(obj):\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown metric function: 'iou_score'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "model = load_model('models/LunarModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "# function to predict result \n",
    "def predict_image(img_path, mask_path, model):\n",
    "    H = 480\n",
    "    W = 480\n",
    "    num_classes = 4\n",
    "\n",
    "    img = imread(img_path)\n",
    "    img = img[:480, :480, :]\n",
    "    img = img / 255.0\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    ## Read mask\n",
    "    mask = imread(mask_path, as_gray = True)\n",
    "    mask = mask[:480, :480]\n",
    "    \n",
    "    ## Prediction\n",
    "    pred_mask = model.predict(np.expand_dims(img, axis=0))\n",
    "    pred_mask = np.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[0]\n",
    "    \n",
    "    \n",
    "    # calculating IOU score\n",
    "    inter = np.logical_and(mask, pred_mask)\n",
    "    union = np.logical_or(mask, pred_mask)\n",
    "    \n",
    "    iou = inter.sum() / union.sum()\n",
    "\n",
    "    return img, mask, pred_mask, iou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
